{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPL \n",
    "\n",
    "# Model interpretability via SHAP values\n",
    "\n",
    "<br>\n",
    "<hr>\n",
    "\n",
    "<br>\n",
    "\n",
    "<h1 style=\"color:red\"> Index</h1>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 0. Setup\n",
    "## 1. Dataset Selection\n",
    "## 2. Model Selection\n",
    "## 3. Calculate SHAP values\n",
    "## 4. Visualization\n",
    "- ### 4.1 Explainability at a Local Level (specific project)\n",
    "- ### 4.2 Explainability at a Global Level\n",
    "- ### 4.3 Importance Ranking\n",
    "- ### 4.4 Explainability between Features\n",
    "\n",
    "<br>\n",
    "<hr>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set root folder (needed for imports)\n",
    "import os\n",
    "while not os.getcwd().endswith(\"ppl-model\"):\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "\n",
    "# larger page width\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "\n",
    "# detect changes on folders\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# ignore warning on jupyter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# my imports\n",
    "import settings\n",
    "from src.modeling import util\n",
    "from src.modeling.models import nnet\n",
    "from dskc import dskc_terminal\n",
    "from dskc import dskc_util\n",
    "\n",
    "# third party imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "import pickle\n",
    "\n",
    "# see all rows of pandas dataframe\n",
    "pd.set_option('display.max_rows', None) # option to see all rows\n",
    "pd.set_option('display.max_columns', None)  # option to see all columns\n",
    "pd.options.display.float_format = '{:,}'.format #to display floats with comma separators \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 1. Dataset Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, feature_names = util.read_train_data()\n",
    "x_test, y_test, test_feature_names = util.read_test_data()\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "x_test_df = pd.DataFrame(x_test[:1000],columns=feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> \n",
    "\n",
    "## 2. Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nnet.load_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 3. Calculate SHAP values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg =  x_train[:20000]\n",
    "\n",
    "# set explainer\n",
    "print(\"\\nCalculating shap explainer...\") \n",
    "timer = dskc_util.Timer()\n",
    "shap_explainer = shap.DeepExplainer(model,bg)\n",
    "timer.end()\n",
    "\n",
    "# save\n",
    "with open('bg.pkl', 'wb') as f:\n",
    "    pickle.dump(bg, f)\n",
    "    \n",
    "# explainer expected value\n",
    "expected_value = shap_explainer.expected_value[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Get SHAP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc shap\n",
    "print(\"\\nCalculating shap values for test...\") \n",
    "timer = dskc_util.Timer()\n",
    "shap_values = shap_explainer.shap_values(x_test[:1000])\n",
    "timer.end()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 4. Visualization\n",
    "\n",
    "\n",
    "The probabilty predicted by the model can be expressed with the following formula:\n",
    "\n",
    "$$P(\\textit{financed}) = \\textit{base value} + \\sum_{i=1}^{74} SHAP_{i}$$\n",
    "\n",
    "\n",
    "A very intuive explanation can be found [here](https://medium.com/@gabrieltseng/interpreting-complex-models-with-shap-values-1c187db6ec83).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SHAP Base value\", expected_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 4.1 Explainability at a Local Level (specific project)\n",
    "\n",
    "The explanation below shows for a **single entry** the contributing features to push the model output from the **base value** (the average model output over the training dataset we passed) towards one. **For this entry there are features pushing** the class label **up**, they are shown in red. And there also are features pushing the class label **down**, they are shown in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set project index\n",
    "index = 10\n",
    "\n",
    "# init the JS visualization code\n",
    "shap.initjs()\n",
    "\n",
    "# predticion value\n",
    "predicted_value = model.predict(x_test)[index][0]\n",
    "print(\"Predicted value:\",predicted_value)\n",
    "\n",
    "# plot the explanation of the first prediction\n",
    "# Note the model is \"multi-output\" because it is rank-2 but only has one column\n",
    "shap.force_plot(shap_explainer.expected_value[0], shap_values[0][index], feature_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Detailed SHAP values (ascending order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = pd.DataFrame(shap_values[0][index], index=feature_names, columns=['SHAP Values']).sort_values(by=['SHAP Values'], ascending=False)\n",
    "aux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 4.2 Explainability at a Global Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the training set predictions\n",
    "shap.force_plot(shap_explainer.expected_value, shap_values[0], x_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 4.3 Importance Ranking\n",
    "\n",
    "We can also just take the mean absolute value of the SHAP values for each feature to get a standard bar plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize the effects of all the features\n",
    "shap.summary_plot(shap_values, x_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 4.4 Explainability between Features\n",
    "\n",
    "\n",
    "To understand how a single feature affects the output of the model we can plot the SHAP value of that feature vs the value of the feature for all the examples in the dataset. Since SHAP values represent a feature's responsibility for a change in the model output, the plot below represents the change in `be financed probability` as feature changes. \n",
    "\n",
    "Vertical dispersion at a single level represents interaction effects with other features. To help reveal these interactions, we used a second feature for for coloring. \n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features\n",
    "feature_1 = \"percentage_days_elapsed\"\n",
    "feature_2 = \"percentage_raised\"\n",
    "\n",
    "# plot\n",
    "shap.dependence_plot(feature_1, shap_values[0], x_test_df[:1000],interaction_index =feature_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features\n",
    "feature_1 = \"percentage_raised\"\n",
    "feature_2 = \"channel\"\n",
    "\n",
    "# plot\n",
    "shap.dependence_plot(feature_1, shap_values[0], x_test_df[:1000],interaction_index =feature_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
